{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa39978b",
   "metadata": {},
   "source": [
    "# Stratified Propensity Score Matching Baseline \n",
    "\n",
    "### Propensity Score Model \n",
    "- Input: Same embeddings used in matching \n",
    "- Model: \n",
    "    - Logistic regression P(T|E) where T is binary treatment and E is the embeddings\n",
    "    - Cross-fitting with cross validation for the regularizer \n",
    "- Ouput: Inference scores for each document that are the propensity scores \n",
    "    \n",
    "### Causal Estimator: Stratified Propensity Score Matching  \n",
    "- Input: Inference scores for each doc (above) \n",
    "- Approach: 5 strata, matching causal estimator \n",
    "- Output: Causal ATE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "577b56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.calibration import calibration_curve\n",
    "from copy import deepcopy\n",
    "import pprint\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1290c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create synthetic toy data just to make sure code runs \n",
    "# Raymond--you'll replace this with the real data \n",
    "nT1 = 100 #number of T=1 units\n",
    "nT0 = 300 #number of T=0 units \n",
    "n_total = nT1 + nT0\n",
    "T = np.array([1]*nT1 + [0]*nT0) #array with treatment values \n",
    "np.mean(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01569556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 50 #dimensionality of embeddings \n",
    "E = np.random.randn(n_total, embedding_dim)\n",
    "E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b36ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "docids = np.arange(n_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e05cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert E.shape[0] == T.shape[0] == n_total == docids.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b9713",
   "metadata": {},
   "source": [
    "### Propensity Score Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fefcdfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CROSSFIT_SPLITS = 2\n",
    "NUM_CROSSVAL_SPLITS = 4 \n",
    "RANDOM_STATE = 42 #for replication \n",
    "\n",
    "CLASSIFIER = Pipeline([(\n",
    "            \"model\",\n",
    "            LogisticRegression(\n",
    "                l1_ratio=0.1,\n",
    "                solver=\"saga\",\n",
    "                max_iter=20000,\n",
    "                tol=1e-3, \n",
    "                penalty=\"elasticnet\",\n",
    "                dual=False,\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=42,\n",
    "                ),),])\n",
    "\n",
    "CLASSIFIER_GRID = {\n",
    "    \"model__C\": [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a97104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossfit num= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katherinekeith/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/katherinekeith/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/katherinekeith/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossfit num= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katherinekeith/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Note: May take awhile to run\n",
    "prob_t_training = np.array([np.nan]*n_total) #only for reporting training acc numbers \n",
    "prob_t_inference = np.array([np.nan]*n_total) #array to save predicted propensity scores \n",
    "# Crossfitting \n",
    "crossfit_split = list(StratifiedKFold(n_splits=NUM_CROSSFIT_SPLITS, shuffle=True, random_state=RANDOM_STATE).split(E, T))\n",
    "for crossfit_number, (train_inds, test_inds) in enumerate(crossfit_split):\n",
    "    print('Crossfit num=', crossfit_number)\n",
    "    # Training \n",
    "    # Cross validation for the training split \n",
    "    inner_cv = StratifiedShuffleSplit(\n",
    "                n_splits=NUM_CROSSVAL_SPLITS,\n",
    "                test_size=1/NUM_CROSSVAL_SPLITS,\n",
    "                random_state= RANDOM_STATE,\n",
    "            )\n",
    "    \n",
    "    t_model_gridsearch = GridSearchCV(\n",
    "                        estimator=deepcopy(CLASSIFIER),\n",
    "                        param_grid=deepcopy(CLASSIFIER_GRID),\n",
    "                        cv=inner_cv,\n",
    "                        scoring=\"roc_auc\",\n",
    "                        refit=True,\n",
    "                    )\n",
    "    \n",
    "    t_model_gridsearch.fit(E[train_inds], T[train_inds])\n",
    "    prob_t_training[train_inds] = t_model_gridsearch.predict_proba(E[train_inds])[:, 1]\n",
    "        \n",
    "    # Inference\n",
    "    # Probability of T=1\n",
    "    prop_t = t_model_gridsearch.predict_proba(E[test_inds])[:, 1]\n",
    "    prob_t_inference[test_inds] = prop_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a186a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all items get inference predictions \n",
    "assert np.mean(np.isnan(prob_t_training)) == 0.0\n",
    "assert np.mean(np.isnan(prob_t_inference)) == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b4fb2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'treatment_model--acc': 0.705,\n",
      " 'treatment_model--ave_prec': 0.5062102030384338,\n",
      " 'treatment_model--calibration_rmse': 0.20752614108345802,\n",
      " 'treatment_model--f1': 0.5530303030303031,\n",
      " 'treatment_model--mean_hard_pred': 0.41,\n",
      " 'treatment_model--mean_soft_pred': 0.4546036283311568,\n",
      " 'treatment_model--mean_true': 0.25,\n",
      " 'treatment_model--roc_auc': 0.7799}\n"
     ]
    }
   ],
   "source": [
    "# Training metrics \n",
    "pred_model_report_out = {}\n",
    "\n",
    "def mean_predictions(dummy, y_pred):\n",
    "    \"\"\"\n",
    "    Helpful for error diagnosing. Returns the mean of the predcted values\n",
    "    Args:\n",
    "        - dummy : we need this arg so that this function looks the same as\n",
    "        sklearn error metrics that take inputs y_true, y_pred\n",
    "        - y_pred : np.array\n",
    "    \"\"\"\n",
    "    return np.mean(y_pred)\n",
    "\n",
    "def calibration_rmse(y_true, y_pred): \n",
    "    \"\"\"\n",
    "    Calculates calibration root mean squared error (RMSE). \n",
    "    Calibration is the extent to which a model's probabilistic predictions match their \n",
    "    corresponding empirical frequencies. \n",
    "    See Nguyen and O'Connor 2015's introduction and definitions \n",
    "    https://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP182.pdf\n",
    "    \"\"\"\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=10, strategy='uniform')\n",
    "    rms = mean_squared_error(prob_true, prob_pred, squared=False) #False returns RMSE vs MSE \n",
    "    return rms\n",
    "\n",
    "\n",
    "def mean_truth(y_true, dummy):\n",
    "    \"\"\"\n",
    "    Helpful for error diagnosing. Returns the mean of the true values\n",
    "    Args:\n",
    "        - y_true : np.array\n",
    "        - dummy : we need this arg so that this function looks the same as\n",
    "        sklearn error metrics that take inputs y_true, y_pred\n",
    "    \"\"\"\n",
    "    return np.mean(y_true)\n",
    "\n",
    "# these classification metrics need the \"hard\" predictions, e.g. y=[0, 0, 1, ...]\n",
    "class_hard_name2metric_func = {\n",
    "    \"f1\": f1_score,\n",
    "    \"acc\": accuracy_score,\n",
    "    \"mean_hard_pred\": mean_predictions,\n",
    "    \"mean_true\": mean_truth,  # should be same for hard or soft\n",
    "}\n",
    "\n",
    "# these classification metrics need the \"score\" predictions, e.g. y=[0.6, 0.77, 0.2, ...]\n",
    "class_scores_name2metric_func = {\n",
    "    \"roc_auc\": roc_auc_score,\n",
    "    \"ave_prec\": average_precision_score,\n",
    "    \"calibration_rmse\": calibration_rmse,\n",
    "    \"mean_soft_pred\": mean_predictions,\n",
    "    \"mean_true\": mean_truth,  # should be same for hard or soft\n",
    "}\n",
    "\n",
    "#hard classifications\n",
    "t_pred_hard = (prob_t_training > 0.5).astype(int)\n",
    "\n",
    "assert t_pred_hard.shape == prob_t_training.shape == T.shape\n",
    "\n",
    "str_sep = \"--\"\n",
    "for metric_str, metric_func in class_hard_name2metric_func.items():\n",
    "    pred_model_report_out[\"treatment_model\" + str_sep + metric_str] = metric_func(T, t_pred_hard)\n",
    "for metric_str, metric_func in class_scores_name2metric_func.items():\n",
    "    pred_model_report_out[\"treatment_model\" + str_sep + metric_str] = metric_func(T, prob_t_training)\n",
    "\n",
    "pprint.pprint(pred_model_report_out)\n",
    "\n",
    "#TODO: Could do this for cross-validation splits as well "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad849b4",
   "metadata": {},
   "source": [
    "### Causal Estimator: Stratified Propensity Score Matching  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5e2282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STRATA = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a93aa8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_strata(prob_t_score, num_strata): \n",
    "    \"\"\"\n",
    "    Reports the strata given the probability score \n",
    "    \n",
    "    Strata index starts at 1 \n",
    "    \"\"\"\n",
    "    strata = np.arange(0, 1.0, 1.0/num_strata)\n",
    "    return np.digitize(prob_t_score, strata, right=False)\n",
    "\n",
    "#small toy test case \n",
    "get_strata(np.array([0.3, 0.6, 0.9]), num_strata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f3a3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "docid2strata = {}\n",
    "for docid, prob_t_score in zip(docids, prob_t_inference): \n",
    "    strata_inferred = get_strata(prob_t_score, NUM_STRATA)\n",
    "    docid2strata[docid] = strata_inferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbfc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Put this in the same matching estimator "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
